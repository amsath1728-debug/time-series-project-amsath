{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "time_series_forcasting.ipynb",
      "authorship_tag": "ABX9TyOqZgif6TAYefo0ISR24FI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsath1728-debug/time-series-project-amsath/blob/main/time_series_forcasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8NgkZ1sWwb2",
        "outputId": "73bf31b5-6591-4652-9350-4487c6bdb644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Advanced Time Series Forecasting ===\n",
            "1. Generating dataset...\n",
            "2. Creating features...\n",
            "3. Preparing data sequences...\n",
            "4. Training Attention LSTM...\n",
            "Epoch 1/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 393ms/step - loss: 19.9814 - mae: 3.6926 - val_loss: 19.5725 - val_mae: 3.6250 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 369ms/step - loss: 20.4732 - mae: 3.7498 - val_loss: 15.3278 - val_mae: 3.1483 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 375ms/step - loss: 11.9575 - mae: 2.7840 - val_loss: 16.7428 - val_mae: 3.3918 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 376ms/step - loss: 7.6658 - mae: 2.1332 - val_loss: 8.4254 - val_mae: 2.3213 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 382ms/step - loss: 7.0269 - mae: 2.0817 - val_loss: 4.8741 - val_mae: 1.7576 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 386ms/step - loss: 6.1607 - mae: 1.9943 - val_loss: 5.3143 - val_mae: 1.8131 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 378ms/step - loss: 6.4771 - mae: 2.0214 - val_loss: 5.2703 - val_mae: 1.8449 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 374ms/step - loss: 5.6562 - mae: 1.8629 - val_loss: 4.7255 - val_mae: 1.7098 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 383ms/step - loss: 4.6851 - mae: 1.7000 - val_loss: 4.5511 - val_mae: 1.6532 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 371ms/step - loss: 5.2859 - mae: 1.7800 - val_loss: 5.8031 - val_mae: 1.9723 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 369ms/step - loss: 5.3705 - mae: 1.7908 - val_loss: 4.4374 - val_mae: 1.6496 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 401ms/step - loss: 4.6837 - mae: 1.6693 - val_loss: 5.5160 - val_mae: 1.8661 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 378ms/step - loss: 4.8300 - mae: 1.7278 - val_loss: 4.2540 - val_mae: 1.6626 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 380ms/step - loss: 4.4728 - mae: 1.6488 - val_loss: 4.8142 - val_mae: 1.7550 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 374ms/step - loss: 4.9180 - mae: 1.7123 - val_loss: 5.0579 - val_mae: 1.7896 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 369ms/step - loss: 5.0908 - mae: 1.7522 - val_loss: 4.5175 - val_mae: 1.6664 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 374ms/step - loss: 4.9682 - mae: 1.7837 - val_loss: 4.6089 - val_mae: 1.6663 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 374ms/step - loss: 4.8518 - mae: 1.7013 - val_loss: 4.4115 - val_mae: 1.6408 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 395ms/step - loss: 5.0824 - mae: 1.7538 - val_loss: 5.0751 - val_mae: 1.7656 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 377ms/step - loss: 4.4726 - mae: 1.6311 - val_loss: 4.6075 - val_mae: 1.7234 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 371ms/step - loss: 4.5093 - mae: 1.6693 - val_loss: 4.7321 - val_mae: 1.6920 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 373ms/step - loss: 4.7471 - mae: 1.6858 - val_loss: 4.6245 - val_mae: 1.6819 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 374ms/step - loss: 4.3898 - mae: 1.5649 - val_loss: 6.2911 - val_mae: 2.0351 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 387ms/step - loss: 4.3003 - mae: 1.5973 - val_loss: 4.2930 - val_mae: 1.6201 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 376ms/step - loss: 4.1315 - mae: 1.5845 - val_loss: 4.2544 - val_mae: 1.6021 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 374ms/step - loss: 4.4619 - mae: 1.6086 - val_loss: 4.4067 - val_mae: 1.6764 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 369ms/step - loss: 4.2074 - mae: 1.5982 - val_loss: 4.3704 - val_mae: 1.6697 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 366ms/step - loss: 4.6342 - mae: 1.6471 - val_loss: 4.3722 - val_mae: 1.6255 - learning_rate: 5.0000e-04\n",
            "5. Training Standard LSTM...\n",
            "Epoch 1/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 196ms/step - loss: 20.3578 - mae: 3.6786 - val_loss: 4.6327 - val_mae: 1.6567\n",
            "Epoch 2/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - loss: 5.2748 - mae: 1.8034 - val_loss: 4.5156 - val_mae: 1.6546\n",
            "Epoch 3/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 185ms/step - loss: 4.8640 - mae: 1.6582 - val_loss: 4.7729 - val_mae: 1.7549\n",
            "Epoch 4/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 178ms/step - loss: 4.5632 - mae: 1.6531 - val_loss: 4.0643 - val_mae: 1.5989\n",
            "Epoch 5/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - loss: 4.1931 - mae: 1.5857 - val_loss: 4.1151 - val_mae: 1.5870\n",
            "Epoch 6/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - loss: 4.5231 - mae: 1.6209 - val_loss: 4.0311 - val_mae: 1.5827\n",
            "Epoch 7/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 4.0671 - mae: 1.5680 - val_loss: 3.9715 - val_mae: 1.5553\n",
            "Epoch 8/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - loss: 4.1424 - mae: 1.6009 - val_loss: 5.1607 - val_mae: 1.7876\n",
            "Epoch 9/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 154ms/step - loss: 4.4167 - mae: 1.6485 - val_loss: 3.9651 - val_mae: 1.5734\n",
            "Epoch 10/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 183ms/step - loss: 4.2310 - mae: 1.5988 - val_loss: 4.0073 - val_mae: 1.5785\n",
            "Epoch 11/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 3.9580 - mae: 1.5267 - val_loss: 4.2517 - val_mae: 1.6393\n",
            "Epoch 12/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 182ms/step - loss: 3.8669 - mae: 1.5219 - val_loss: 4.0854 - val_mae: 1.6075\n",
            "Epoch 13/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - loss: 3.9520 - mae: 1.5442 - val_loss: 4.0803 - val_mae: 1.5859\n",
            "Epoch 14/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - loss: 3.8019 - mae: 1.4923 - val_loss: 4.1615 - val_mae: 1.6099\n",
            "Epoch 15/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step - loss: 4.1258 - mae: 1.5385 - val_loss: 4.0640 - val_mae: 1.6246\n",
            "Epoch 16/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 178ms/step - loss: 4.0917 - mae: 1.5492 - val_loss: 3.9664 - val_mae: 1.5664\n",
            "Epoch 17/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 4.5716 - mae: 1.6163 - val_loss: 3.9804 - val_mae: 1.5689\n",
            "Epoch 18/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - loss: 3.8959 - mae: 1.5288 - val_loss: 3.9823 - val_mae: 1.5714\n",
            "Epoch 19/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - loss: 3.9971 - mae: 1.5378 - val_loss: 4.1445 - val_mae: 1.6094\n",
            "Epoch 20/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 170ms/step - loss: 3.6685 - mae: 1.4942 - val_loss: 4.0220 - val_mae: 1.5988\n",
            "Epoch 21/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 155ms/step - loss: 3.8593 - mae: 1.4926 - val_loss: 4.0996 - val_mae: 1.6117\n",
            "Epoch 22/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 154ms/step - loss: 3.8863 - mae: 1.5117 - val_loss: 4.0250 - val_mae: 1.5950\n",
            "Epoch 23/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 153ms/step - loss: 3.6212 - mae: 1.4521 - val_loss: 3.8477 - val_mae: 1.5589\n",
            "Epoch 24/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 169ms/step - loss: 3.7351 - mae: 1.4961 - val_loss: 4.5905 - val_mae: 1.7293\n",
            "Epoch 25/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 181ms/step - loss: 3.4266 - mae: 1.4456 - val_loss: 4.2775 - val_mae: 1.6257\n",
            "Epoch 26/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 177ms/step - loss: 3.8935 - mae: 1.5274 - val_loss: 4.2307 - val_mae: 1.6425\n",
            "Epoch 27/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 3.4479 - mae: 1.4429 - val_loss: 4.0669 - val_mae: 1.6044\n",
            "Epoch 28/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 179ms/step - loss: 3.9311 - mae: 1.4786 - val_loss: 4.2054 - val_mae: 1.6393\n",
            "Epoch 29/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 3.5904 - mae: 1.4722 - val_loss: 3.9296 - val_mae: 1.5752\n",
            "Epoch 30/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 179ms/step - loss: 3.6004 - mae: 1.4748 - val_loss: 4.0892 - val_mae: 1.6121\n",
            "Epoch 31/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 3.3564 - mae: 1.4235 - val_loss: 4.0014 - val_mae: 1.6127\n",
            "Epoch 32/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 183ms/step - loss: 3.6238 - mae: 1.4553 - val_loss: 3.8779 - val_mae: 1.5630\n",
            "Epoch 33/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 3.2726 - mae: 1.3963 - val_loss: 4.2164 - val_mae: 1.6491\n",
            "Epoch 34/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 185ms/step - loss: 3.6284 - mae: 1.4636 - val_loss: 4.3353 - val_mae: 1.6692\n",
            "Epoch 35/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 180ms/step - loss: 3.4601 - mae: 1.4501 - val_loss: 4.1930 - val_mae: 1.6491\n",
            "Epoch 36/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - loss: 3.7998 - mae: 1.4973 - val_loss: 4.1349 - val_mae: 1.6284\n",
            "Epoch 37/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 195ms/step - loss: 3.3679 - mae: 1.4477 - val_loss: 4.0834 - val_mae: 1.6444\n",
            "Epoch 38/100\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 182ms/step - loss: 3.8617 - mae: 1.4982 - val_loss: 4.1794 - val_mae: 1.6341\n",
            "6. Evaluating models...\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
            "\n",
            "=== MODEL PERFORMANCE ===\n",
            "                    RMSE       MAE        MAPE\n",
            "attention_lstm  3.071632  2.561702  398.169714\n",
            "standard_lstm   2.029148  1.586361  188.180350\n"
          ]
        }
      ],
      "source": [
        "# filename: time_series_forecasting.py\n",
        "\"\"\"\n",
        "Advanced Time Series Forecasting with Attention Mechanisms\n",
        "Complete implementation with data generation, model training, and evaluation\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DataGenerator:\n",
        "    \"\"\"Generate complex multivariate time series dataset\"\"\"\n",
        "\n",
        "    def generate_dataset(self, n_obs=2000):\n",
        "        \"\"\"Create synthetic dataset with seasonality and volatility\"\"\"\n",
        "        np.random.seed(42)\n",
        "        dates = pd.date_range(start='2020-01-01', periods=n_obs, freq='H')\n",
        "        t = np.arange(n_obs)\n",
        "\n",
        "        # Multiple seasonal patterns\n",
        "        trend = 0.001 * t + 10 * np.sin(0.0001 * t)\n",
        "        daily_seasonality = 5 * np.sin(2 * np.pi * t / 24)\n",
        "        weekly_seasonality = 3 * np.sin(2 * np.pi * t / (24 * 7))\n",
        "\n",
        "        # Volatility clustering (GARCH-like)\n",
        "        volatility = np.ones(n_obs)\n",
        "        returns = np.zeros(n_obs)\n",
        "\n",
        "        for i in range(1, n_obs):\n",
        "            volatility[i] = 0.1 + 0.7 * volatility[i-1] + 0.2 * returns[i-1]**2\n",
        "            returns[i] = np.random.normal(0, np.sqrt(volatility[i]))\n",
        "\n",
        "        main_series = trend + daily_seasonality + weekly_seasonality + 2 * returns\n",
        "        series_2 = 0.7 * main_series + 0.3 * np.random.normal(0, 1, n_obs)\n",
        "        series_3 = 0.5 * main_series + 0.2 * series_2 + 0.3 * np.random.normal(0, 1, n_obs)\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'timestamp': dates,\n",
        "            'main_series': main_series,\n",
        "            'correlated_series_1': series_2,\n",
        "            'correlated_series_2': series_3,\n",
        "            'volatility': volatility\n",
        "        })\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        return df\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Create comprehensive features for time series forecasting\"\"\"\n",
        "\n",
        "    def create_features(self, df, target_col='main_series'):\n",
        "        \"\"\"Generate lagged, statistical, and seasonal features\"\"\"\n",
        "        features_df = df.copy()\n",
        "\n",
        "        # Lagged features\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48, 168]:\n",
        "            features_df[f'lag_{lag}'] = features_df[target_col].shift(lag)\n",
        "\n",
        "        # Moving statistics\n",
        "        for window in [24, 168]:\n",
        "            features_df[f'ma_{window}'] = features_df[target_col].rolling(window).mean()\n",
        "            features_df[f'std_{window}'] = features_df[target_col].rolling(window).std()\n",
        "\n",
        "        # Fourier terms for seasonality\n",
        "        features_df['fourier_sin_daily'] = np.sin(2 * np.pi * features_df.index.hour / 24)\n",
        "        features_df['fourier_cos_daily'] = np.cos(2 * np.pi * features_df.index.hour / 24)\n",
        "        features_df['fourier_sin_weekly'] = np.sin(2 * np.pi * features_df.index.dayofweek / 7)\n",
        "        features_df['fourier_cos_weekly'] = np.cos(2 * np.pi * features_df.index.dayofweek / 7)\n",
        "\n",
        "        # Time features\n",
        "        features_df['hour'] = features_df.index.hour\n",
        "        features_df['day_of_week'] = features_df.index.dayofweek\n",
        "\n",
        "        return features_df.dropna()\n",
        "\n",
        "class AttentionLSTMModel(Model):\n",
        "    \"\"\"LSTM with self-attention mechanism for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, units=64, attention_heads=4, dropout_rate=0.2, output_steps=1):\n",
        "        super(AttentionLSTMModel, self).__init__()\n",
        "        self.units = units\n",
        "        self.lstm = LSTM(units, return_sequences=True)\n",
        "        self.attention = MultiHeadAttention(num_heads=attention_heads, key_dim=units)\n",
        "        self.layer_norm1 = LayerNormalization()\n",
        "        self.layer_norm2 = LayerNormalization()\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(units * 2, activation='relu'),\n",
        "            Dropout(dropout_rate),\n",
        "            Dense(units)\n",
        "        ])\n",
        "        self.global_pool = GlobalAveragePooling1D()\n",
        "        self.output_layer = Dense(output_steps)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs)\n",
        "        attn_output = self.attention(x, x)\n",
        "        x = self.layer_norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layer_norm2(x + ffn_output)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "class StandardLSTMModel:\n",
        "    \"\"\"Baseline LSTM model without attention\"\"\"\n",
        "\n",
        "    def create_model(self, input_shape, units=50):\n",
        "        model = tf.keras.Sequential([\n",
        "            LSTM(units, return_sequences=True, input_shape=input_shape),\n",
        "            Dropout(0.2),\n",
        "            LSTM(units),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "class ForecastingPipeline:\n",
        "    \"\"\"Complete pipeline for model training and evaluation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_sequences(self, features_df, target_col='main_series', seq_length=168):\n",
        "        \"\"\"Create sequences for deep learning models\"\"\"\n",
        "        feature_cols = [col for col in features_df.columns if col != target_col]\n",
        "        X = self.scaler.fit_transform(features_df[feature_cols])\n",
        "        y = features_df[target_col].values\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "        for i in range(seq_length, len(X)):\n",
        "            X_seq.append(X[i-seq_length:i])\n",
        "            y_seq.append(y[i])\n",
        "\n",
        "        X_seq = np.array(X_seq)\n",
        "        y_seq = np.array(y_seq)\n",
        "\n",
        "        # Train-test split (80-20)\n",
        "        split_idx = int(0.8 * len(X_seq))\n",
        "        X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "        y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "        # Validation split (10% of training)\n",
        "        val_split = int(0.9 * len(X_train))\n",
        "        X_val, y_val = X_train[val_split:], y_train[val_split:]\n",
        "        X_train, y_train = X_train[:val_split], y_train[:val_split]\n",
        "\n",
        "        return (X_train, y_train, X_val, y_val, X_test, y_test), feature_cols\n",
        "\n",
        "    def train_attention_lstm(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train the attention-based model\"\"\"\n",
        "        model = AttentionLSTMModel()\n",
        "        model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            callbacks=[\n",
        "                tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "            ]\n",
        "        )\n",
        "        self.models['attention_lstm'] = model\n",
        "        return history\n",
        "\n",
        "    def train_standard_lstm(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train baseline LSTM model\"\"\"\n",
        "        model = StandardLSTMModel().create_model(X_train.shape[1:])\n",
        "        model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)]\n",
        "        )\n",
        "        self.models['standard_lstm'] = model\n",
        "        return history\n",
        "\n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        \"\"\"Compare model performance\"\"\"\n",
        "        metrics = {}\n",
        "        for name, model in self.models.items():\n",
        "            y_pred = model.predict(X_test).flatten()\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "            metrics[name] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
        "\n",
        "        self.results = metrics\n",
        "        return metrics\n",
        "\n",
        "def main():\n",
        "    \"\"\"Execute complete forecasting pipeline\"\"\"\n",
        "    print(\"=== Advanced Time Series Forecasting ===\")\n",
        "\n",
        "    # Generate data\n",
        "    print(\"1. Generating dataset...\")\n",
        "    data_gen = DataGenerator()\n",
        "    df = data_gen.generate_dataset(2000)\n",
        "\n",
        "    # Feature engineering\n",
        "    print(\"2. Creating features...\")\n",
        "    feature_engineer = FeatureEngineer()\n",
        "    features_df = feature_engineer.create_features(df)\n",
        "\n",
        "    # Prepare pipeline\n",
        "    print(\"3. Preparing data sequences...\")\n",
        "    pipeline = ForecastingPipeline()\n",
        "    (X_train, y_train, X_val, y_val, X_test, y_test), feature_names = pipeline.prepare_sequences(features_df)\n",
        "\n",
        "    # Train models\n",
        "    print(\"4. Training Attention LSTM...\")\n",
        "    pipeline.train_attention_lstm(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    print(\"5. Training Standard LSTM...\")\n",
        "    pipeline.train_standard_lstm(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"6. Evaluating models...\")\n",
        "    metrics = pipeline.evaluate_models(X_test, y_test)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n=== MODEL PERFORMANCE ===\")\n",
        "    results_df = pd.DataFrame(metrics).T\n",
        "    print(results_df)\n",
        "\n",
        "    return pipeline, metrics, features_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline, metrics, features_df = main()"
      ]
    }
  ]
}